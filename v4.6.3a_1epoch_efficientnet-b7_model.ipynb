{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a480142-31a7-45f3-8499-5fe7e84b8964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.17 (main, Jul  5 2023, 20:41:20) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a29350d-ce11-477e-a10b-4812d87a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549cb5b9-cc34-4d37-9684-ea23b351cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e814553-36ed-44f1-8b64-bf248f9798d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.18\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "print(fastai.__version__)  # Check the installed version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9b43419-e287-4760-9e84-fea7ebaf0cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from fastai.vision.all import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the DataBlock\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),  # Input: Image, Target: Category\n",
    "    get_items=get_image_files,  # Get all image files in the path\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),  # Train/valid split\n",
    "    get_y=parent_label,  # Labels from parent folder names\n",
    "    item_tfms=Resize(460),  # Resize to 460 before batching\n",
    "    batch_tfms=aug_transforms(size=224)  # Resize to 224 with augmentations\n",
    ")\n",
    "\n",
    "# Load the DataLoaders\n",
    "path = Path('data/train19')\n",
    "dls = dblock.dataloaders(path, bs=8, num_workers=0)\n",
    "\n",
    "# Show a batch of images\n",
    "#dls.show_batch(max_n=9, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d03033-d9d2-4204-8880-b9d2e9a65298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Later on set as a function - Hard coded from the excel train 2015 Let's say these are the sample counts for each class\n",
    "#class_counts = [25810, 2443, 5292, 873, 708]  # Replace with your actual class distribution\n",
    "class_counts = [1805, 370, 999, 193, 295]  # Replace with your actual class distribution\n",
    "# Compute weights inversely proportional to class frequency\n",
    "class_weights = [1.0 / count for count in class_counts]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def get_sample_weights(dls, class_weights):\n",
    "    sample_weights = []\n",
    "    for idx in range(len(dls.train_ds)):\n",
    "        label = dls.train_ds[idx][1]  # Get the label of each sample\n",
    "        sample_weights.append(class_weights[label])\n",
    "    return torch.tensor(sample_weights, dtype=torch.float32)\n",
    "\n",
    "# Compute sample weights\n",
    "sample_weights = get_sample_weights(dls, class_weights)\n",
    "\n",
    "weighted_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights, \n",
    "    num_samples=len(sample_weights),  # Number of samples in an epoch\n",
    "    replacement=True  # Sampling with replacement\n",
    ")\n",
    "\n",
    "# Replace the default train DataLoader's sampler\n",
    "dls.train.loader = torch.utils.data.DataLoader(\n",
    "    dls.train_ds,\n",
    "    batch_size=8,\n",
    "    sampler=weighted_sampler,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26043ab-dfd5-4999-8d9f-a6417e57848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModelMethods:\n",
    "    \"\"\"CustomModelMethods is a base class that defines methods for training and evaluating a model using FastAI.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize without any learner\n",
    "        self.class_learner = None\n",
    "\n",
    "    def get_learner(self, dls, criterion, metrics):\n",
    "        \"\"\"Initializes a FastAI Learner with a given dataset, criterion, and metrics.\"\"\"\n",
    "        if self.class_learner is None:  # If learner doesn't exist, create one\n",
    "            self.class_learner = Learner(dls, self, loss_func=criterion, metrics=metrics)\n",
    "        return self.class_learner  # Return the initialized learner\n",
    "\n",
    "    def train_model(self, dls, epochs=10, criterion=None, early_stopping=None):\n",
    "        \"\"\"Trains the model using FastAI Learner's fine_tune method for transfer learning or training from scratch.\"\"\"\n",
    "        if self.class_learner is None:  # Create learner if not already initialized\n",
    "            # Initialize Learner with loss function\n",
    "            self.class_learner = Learner(dls, self, loss_func=criterion, metrics=accuracy)\n",
    "            self.class_learner.create_opt()  # Create the optimizer\n",
    "\n",
    "        # Train for the specified number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.class_learner.fit_one_cycle(1)  # Train for one epoch\n",
    "            val_loss = self.class_learner.validate()[0]  # Get validation loss\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stopping and early_stopping(val_loss):\n",
    "                print(f\"Early stopping triggered after epoch {epoch + 1}.\")\n",
    "                break  # Exit training loop if early stopping is triggered\n",
    "\n",
    "    def evaluate_model(self, dls):\n",
    "        \"\"\"Evaluates the model: generates confusion matrix, displays top losses, and calculates accuracy.\"\"\"\n",
    "        print(\"Using validation DataLoader for evaluation.\")\n",
    "        print(f\"Validation DataLoader size: {len(dls.valid)}\")\n",
    "\n",
    "        if self.class_learner is None:\n",
    "            self.class_learner = Learner(dls.valid, self, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
    "\n",
    "        # Check dataset sizes during evaluation\n",
    "        print(f\"Validation dataset size: {len(dls.valid_ds)}\")\n",
    "        print(f\"Training dataset size: {len(dls.train_ds)}\")\n",
    "\n",
    "        # Print DataLoader sizes\n",
    "        print(f\"Training DataLoader size: {len(dls.train)}\")\n",
    "        print(f\"Validation DataLoader size: {len(dls.valid)}\")\n",
    "\n",
    "        # Validate structure of the validation dataset\n",
    "        print(\"Validating the structure of the validation dataset...\")\n",
    "        print(f\"Sample validation data (first 5 items): {dls.valid_ds.items[:5]}\")  # Print first 5 items\n",
    "\n",
    "        # Correctly access the validation labels\n",
    "        labels = [item[1] for item in dls.valid_ds.items]  # Assuming the second element is the label\n",
    "        print(f\"Sample validation labels (first 5): {labels[:5]}\")  # Print first 5 labels\n",
    "\n",
    "        # Generate metrics like confusion matrix and top losses\n",
    "        interp = ClassificationInterpretation.from_learner(self.class_learner)\n",
    "\n",
    "        # Get predictions and targets\n",
    "        preds, targets = self.class_learner.get_preds(ds_idx=1)  # Ensures we use validation set\n",
    "\n",
    "        # Check for any mismatch in lengths\n",
    "        if preds.size(0) != targets.size(0):\n",
    "            print(f\"Mismatch detected! Predictions: {preds.size(0)}, Targets: {targets.size(0)}\")\n",
    "            # Truncate predictions or targets to the smaller size\n",
    "            min_size = min(preds.size(0), targets.size(0))\n",
    "            preds = preds[:min_size]\n",
    "            targets = targets[:min_size]\n",
    "            print(f\"Using truncated sizes: {preds.size(0)} for both predictions and targets.\")\n",
    "\n",
    "        # Print the shape of the predictions and targets to diagnose any mismatch\n",
    "        print(f\"Shape of predictions: {preds.shape}\")\n",
    "        print(f\"Shape of targets: {targets.shape}\")\n",
    "\n",
    "        # Print the first few predictions and targets for a quick sanity check\n",
    "        print(f\"Sample predictions: {preds[:10]}\")\n",
    "        print(f\"Sample targets: {targets[:10]}\")\n",
    "\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "        # Check for unique classes in targets and preds\n",
    "        unique_targets = set(targets.numpy())\n",
    "        unique_preds = set(preds.numpy())\n",
    "\n",
    "        if len(unique_targets) == 0 or len(unique_preds) == 0:\n",
    "            print(\"Warning: No valid classes present in targets or predictions.\")\n",
    "        else:\n",
    "            print(f\"Accuracy: {accuracy_score(targets, preds):.4f}\")\n",
    "            print('Classification Report:')\n",
    "            print(classification_report(targets, preds, zero_division=0))  # Adjust zero_division as needed\n",
    "            print('Confusion Matrix:')\n",
    "            print(confusion_matrix(targets, preds))\n",
    "\n",
    "            # Micro and Macro metrics\n",
    "            micro_precision = precision_score(targets, preds, average='micro')\n",
    "            micro_recall = recall_score(targets, preds, average='micro')\n",
    "            micro_f1 = f1_score(targets, preds, average='micro')\n",
    "\n",
    "            macro_precision = precision_score(targets, preds, average='macro')\n",
    "            macro_recall = recall_score(targets, preds, average='macro')\n",
    "            macro_f1 = f1_score(targets, preds, average='macro')\n",
    "\n",
    "            print(\"\\nMicro and Macro Averages:\")\n",
    "            print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "            print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "            print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "            print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "            print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "            print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    def save_model(self, filename, mode='weights', pretrained_pkl_path=None):\n",
    "        \"\"\"Saves the model weights to the specified file or directory using Hugging Face's save_pretrained method.\"\"\"\n",
    "        if mode == 'weights':\n",
    "            if hasattr(self.class_learner.model, 'save_pretrained'):\n",
    "                #Issue fix: Using Hugging Face's save_pretrained method\n",
    "                self.class_learner.model.save_pretrained(filename)  # Use Hugging Face method\n",
    "                print(f\"Hugging Face method - Model weights saved to {filename}.\")\n",
    "            else:\n",
    "                torch.save(self.class_learner.model.state_dict(), filename)  # Fallback for non-Hugging Face models\n",
    "                print(f\"Model weights saved to {filename}.\")\n",
    "        elif mode == 'full':\n",
    "            torch.save(self.class_learner, filename)\n",
    "            print(f\"Full model saved to {filename}.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Use 'full' or 'weights'.\")\n",
    "\n",
    "        # Export to pickle file from the learner\n",
    "        if pretrained_pkl_path is not None:\n",
    "            #Issue fix: Ensure compatibility with Hugging Face models\n",
    "            self.class_learner.export(pretrained_pkl_path)\n",
    "            print(f\"Learner exported to {pretrained_pkl_path}.\")\n",
    "\n",
    "    def load_model(self, filename, mode='full'):\n",
    "        \"\"\"Loads the model weights or the full model from the specified file.\"\"\"\n",
    "        try:\n",
    "            if filename.endswith('.pth') or filename.endswith('.pt'):\n",
    "                if mode == 'weights':\n",
    "                    self.class_learner.model.load_state_dict(torch.load(filename))\n",
    "                    print(f\"Model weights loaded from {filename}.\")\n",
    "                elif mode == 'full':\n",
    "                    self.class_learner = torch.load(filename)  # Load the entire model\n",
    "                    print(f\"Full model loaded from {filename}.\")\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid mode. Use 'full' or 'weights'.\")\n",
    "            elif filename.endswith('.pkl'):\n",
    "                self.class_learner = torch.load(filename)  # Load the entire learner from the pickle file\n",
    "                print(f\"Full model loaded from {filename}.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file extension. Use .pth, .pt, or .pkl.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            if mode == 'weights':\n",
    "                #Issue fix: Attempt loading weights with strict=False for compatibility\n",
    "                self.class_learner.model.load_state_dict(torch.load(filename, strict=False))\n",
    "                print(f\"Model weights loaded with strict=False from {filename}. Warning: {e}\")\n",
    "            elif mode == 'full':\n",
    "                self.class_learner = torch.load(filename, strict=False)\n",
    "                print(f\"Full model loaded with strict=False from {filename}. Warning: {e}\")\n",
    "            else:\n",
    "                print(f\"An error occurred: {e}. Invalid mode. Use 'full' or 'weights'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5503d652-d605-42c6-aac9-4e6d5b851919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedEyeDiseaseClassifier(nn.Module, CustomModelMethods):\n",
    "    \"\"\"A pretrained model for classifying eye diseases using transfer learning. Supports VGG16, ResNet18, or EfficientNet-B7.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=5, pretrained_model='vgg16'):\n",
    "        nn.Module.__init__(self)  # Initialize PyTorch's nn.Module\n",
    "        CustomModelMethods.__init__(self)  # Initialize methods for training/evaluation from the CustomModelMethods class\n",
    "        print(\"Initializing PretrainedEyeDiseaseClassifier...\")\n",
    "\n",
    "        # Initialize num_ftrs as an instance variable\n",
    "        self.num_ftrs = None\n",
    "\n",
    "        # Choose between VGG16, ResNet18, or EfficientNet-B7 pretrained models\n",
    "        if pretrained_model == 'vgg16':\n",
    "            self.model = models.vgg16(pretrained=True)  # Load pretrained VGG16 model\n",
    "            self.num_ftrs = 4096  # Set num_ftrs for VGG16\n",
    "            self.model.classifier[6] = nn.Linear(self.num_ftrs, num_classes)  # Replace final layer for `num_classes`\n",
    "            print(\"Using VGG16 model.\")\n",
    "        elif pretrained_model == 'resnet18':\n",
    "            self.model = models.resnet18(pretrained=True)  # Load pretrained ResNet18 model\n",
    "            self.num_ftrs = self.model.fc.in_features  # Get the number of input features for the final layer\n",
    "            self.model.fc = nn.Linear(self.num_ftrs, num_classes)  # Replace final layer with a custom one\n",
    "            print(\"Using ResNet18 model.\")\n",
    "        elif pretrained_model == 'efficientnet-b7':\n",
    "            self.model = EfficientNet.from_pretrained('efficientnet-b7')  # Load pretrained EfficientNet-B7 model\n",
    "            self.num_ftrs = self.model._fc.in_features  # Get the number of input features for the final layer\n",
    "            self.model._fc = nn.Linear(self.num_ftrs, num_classes)  # Replace final layer with a custom one\n",
    "            print(\"Using EfficientNet-B7 model.\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pretrained model. Choose 'vgg16', 'resnet18', or 'efficientnet-b7'.\")\n",
    "\n",
    "    def create_fc_layers(self, num_classes, num_ftrs):\n",
    "        \"\"\"Creates custom fully connected layers connected to the pretrained model's output.\"\"\"\n",
    "        print(\"Creating fully connected layers...\")\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),  # First fully connected layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(512, 256),  # Second fully connected layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(256, num_classes)  # Final layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass for the model, which applies the pretrained model's forward pass.\n",
    "        every batch processed during training or validation calls the forward method,\n",
    "        and you have a print statement inside that method. so comment it\n",
    "        \"\"\"\n",
    "\n",
    "        ##print(\"Performing forward pass...\")\n",
    "        x = self.model(x)  # Pass input through the model\n",
    "        return x  # Return the output from the model\n",
    "\n",
    "    def set_num_classes(self, num_classes):\n",
    "        \"\"\"Dynamically adjust the final layer to accommodate a new number of classes.\"\"\"\n",
    "        print(\"Setting number of classes...\")\n",
    "        if isinstance(self.model, models.VGG):\n",
    "            self.model.classifier[6] = nn.Linear(self.num_ftrs, num_classes)  # Update VGG final layer\n",
    "        elif isinstance(self.model, models.ResNet):\n",
    "            self.model.fc = nn.Linear(self.num_ftrs, num_classes)  # Update ResNet final layer\n",
    "        elif isinstance(self.model, EfficientNet):\n",
    "            self.model._fc = nn.Linear(self.num_ftrs, num_classes)  # Update EfficientNet final layer\n",
    "\n",
    "        print(f\"Number of classes updated to {num_classes}.\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a7b5819-4d21-498f-865f-c46c611d4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Custom precision with zero_division handling for macro average\n",
    "def precision_macro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return precision_score(targs, preds, average='macro', zero_division=1)\n",
    "\n",
    "def precision_micro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return precision_score(targs, preds, average='micro', zero_division=1)\n",
    "\n",
    "# Custom recall with zero_division handling for macro average\n",
    "def recall_macro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return recall_score(targs, preds, average='macro', zero_division=1)\n",
    "\n",
    "def recall_micro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return recall_score(targs, preds, average='micro', zero_division=1)\n",
    "\n",
    "# Custom f1 score with zero_division handling for macro average\n",
    "def f1score_macro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return f1_score(targs, preds, average='macro', zero_division=1)\n",
    "\n",
    "def f1score_micro(preds, targs):\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targs = targs.cpu().numpy()\n",
    "    return f1_score(targs, preds, average='micro', zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb45c1c-56df-4a94-8d39-74c82869aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping monitors the validation loss during training and stops the training process\n",
    "    if the loss does not improve after a specified number of epochs (patience).\n",
    "\n",
    "    Attributes:\n",
    "    - patience: The number of epochs to wait for improvement before stopping.\n",
    "    - min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "    - counter: Counts the number of epochs without improvement.\n",
    "    - best_loss: Stores the best validation loss encountered during training.\n",
    "    - stopped_epoch: Records the epoch number when training was stopped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.stopped_epoch = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Checks the validation loss to determine if training should stop.\n",
    "\n",
    "        Parameters:\n",
    "        - val_loss: The current validation loss to check against the best loss.\n",
    "\n",
    "        Returns:\n",
    "        - bool: True if training should stop, otherwise False.\n",
    "        \"\"\"\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if there's improvement\n",
    "        else:\n",
    "            self.counter += 1  # Increment counter if no improvement\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            self.stopped_epoch = self.counter\n",
    "            return True  # Stop training\n",
    "\n",
    "        return False  # Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62891e17-b9b3-4f07-9b92-af86f177cf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PretrainedEyeDiseaseClassifier...\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "Using EfficientNet-B7 model.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Model\n",
    "model = PretrainedEyeDiseaseClassifier(pretrained_model='efficientnet-b7', num_classes=5)\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7114b79-bb78-4d8e-a011-8e6b048cd806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_micro</th>\n",
       "      <th>recall_micro</th>\n",
       "      <th>f1score_micro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1score_macro</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.720730</td>\n",
       "      <td>0.658764</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.752732</td>\n",
       "      <td>0.798483</td>\n",
       "      <td>0.657529</td>\n",
       "      <td>0.550026</td>\n",
       "      <td>21:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 0.7207295894622803, Validation Loss: 0.6588\n"
     ]
    }
   ],
   "source": [
    "# Create the FAST AI Learner\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    opt_func=partial(Adam, lr=0.001),\n",
    "    metrics=[\n",
    "        accuracy,\n",
    "        precision_micro, recall_micro, f1score_micro,\n",
    "        precision_macro, recall_macro, f1score_macro\n",
    "    ]\n",
    ")\n",
    "\n",
    "learn.create_opt()\n",
    "# Train the Model\n",
    "epochs= 1\n",
    "already_frozen = False\n",
    "for epoch in range(epochs):\n",
    "    # Train for one epoch\n",
    "    if not already_frozen:\n",
    "        learn.freeze()\n",
    "        already_frozen = True\n",
    "        learn.fit_one_cycle(1)\n",
    "        learn.unfreeze()\n",
    "    else:\n",
    "        learn.fit_one_cycle(1)\n",
    "\n",
    "    # Get training loss directly from the recorder\n",
    "    if learn.recorder.values and len(learn.recorder.values) > 0 and len(learn.recorder.values[-1]) > 0:\n",
    "        train_loss = learn.recorder.values[-1][0]         \n",
    "    else:\n",
    "        train_loss = 'N/A'\n",
    "\n",
    "    val_loss = learn.validate()[0]  # Get validation loss\n",
    "    # Print training and validation losses if available, otherwise indicate 'N/A'\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if early_stopping != None and early_stopping(val_loss):\n",
    "        print(f\"Early stopping triggered after epoch {epoch + 1}.\")            \n",
    "        break  # Exit training loop if early stopping is triggere\n",
    "\n",
    "\n",
    "# # Train the Model\n",
    "# epochs= 1\n",
    "# for epoch in range(epochs):\n",
    "#     # Train for one epoch\n",
    "#     learn.fit_one_cycle(1)\n",
    "\n",
    "#     # Get training loss directly from the recorder\n",
    "#     train_loss = learn.recorder.values[-1][0] if learn.recorder.values else 'N/A'\n",
    "\n",
    "\n",
    "#     val_loss = learn.validate()[0]  # Get validation loss\n",
    "#     # Print training and validation losses if available, otherwise indicate 'N/A'\n",
    "#     print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "#     # Check for early stopping\n",
    "#     if early_stopping and early_stopping(val_loss):\n",
    "#         print(f\"Early stopping triggered after epoch {epoch + 1}.\")\n",
    "#         break  # Exit training loop if early stopping is triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d52cfc-ec50-461f-b8ef-1d1b26d6aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.show_results(max_n=9, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea2086c-2878-4414-8e9e-80e052eeb21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7527\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       352\n",
      "           1       0.53      0.51      0.52        78\n",
      "           2       0.57      0.83      0.67       198\n",
      "           3       1.00      0.00      0.00        37\n",
      "           4       1.00      0.00      0.00        67\n",
      "\n",
      "    accuracy                           0.75       732\n",
      "   macro avg       0.81      0.47      0.43       732\n",
      "weighted avg       0.81      0.75      0.70       732\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[347   3   2   0   0]\n",
      " [ 10  40  28   0   0]\n",
      " [  7  27 164   0   0]\n",
      " [  0   3  34   0   0]\n",
      " [  3   3  61   0   0]]\n",
      "\n",
      "Micro and Macro Averages:\n",
      "Micro Precision: 0.7527\n",
      "Micro Recall: 0.7527\n",
      "Micro F1 Score: 0.7527\n",
      "Macro Precision: 0.4079\n",
      "Macro Recall: 0.4654\n",
      "Macro F1 Score: 0.4316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alon/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Custom function to generate a report after training\n",
    "def evaluate_model(learner, dl):\n",
    "    preds, targs = learner.get_preds(dl=dl)\n",
    "    preds = preds.argmax(dim=1).cpu().numpy()\n",
    "    targets = targs.cpu().numpy()\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(targets, preds, zero_division=1))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(targets, preds))\n",
    "\n",
    "    micro_precision = precision_score(targets, preds, average='micro')\n",
    "    micro_recall = recall_score(targets, preds, average='micro')\n",
    "    micro_f1 = f1_score(targets, preds, average='micro')\n",
    "\n",
    "    macro_precision = precision_score(targets, preds, average='macro')\n",
    "    macro_recall = recall_score(targets, preds, average='macro')\n",
    "    macro_f1 = f1_score(targets, preds, average='macro')\n",
    "\n",
    "    print(\"\\nMicro and Macro Averages:\")\n",
    "    print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "    print(f\"Micro Recall: {micro_recall:.4f}\")\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "    print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Call this function after training, passing the validation dataloader\n",
    "evaluate_model(learn, dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e67644-0a87-47f5-a65b-075f654c9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state_dict (model parameters)\n",
    "# torch.save(model, \"models/v4.6_40epoch_efficientnet-b7_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeedf8eb-b214-4dd2-bb34-bf26a2081814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model saved to models/v4.6.3_efficientnet-b7_model_full.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the state_dict (model parameters)\n",
    "\n",
    "# In any case save in old format\n",
    "torch.save(model, \"models/v4.6.3_efficientnet-b7_model.pth\")\n",
    "\n",
    "# model.save_model(\"models/v4.7_efficientnet-b7_model.pt\", mode='weights')\n",
    "# model.save_model(\"models/v4.7_efficientnet-b7_model.pth\", mode='full', pretrained_pkl_path='models/v4.7_efficientnet-b7_model.pkl')\n",
    "model.save_model(\"models/v4.6.3_efficientnet-b7_model_full.pth\", mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18fea840-3782-4bb9-9379-f79e2f75fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the state_dict (model parameters)        \n",
    "learn.export(\"models/v4.6.3_efficientnet-b7_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
